# === Import Required Libraries ===
import pandas as pd          # for data handling (reading CSV, working with dataframes)
import numpy as np           # for numerical operations (not used here directly but helpful)
import matplotlib.pyplot as plt  # for creating plots/graphs
import seaborn as sns        # advanced graphs (like heatmaps)
from sklearn.tree import DecisionTreeClassifier, plot_tree  # Decision tree classifier and plot
from sklearn.model_selection import train_test_split  # to split data into train/test
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # for evaluation
from sklearn.preprocessing import LabelEncoder  # for encoding the target labels

# === Load Dataset ===
file_path = 'iris.csv'
df = pd.read_csv(file_path)  # Load dataset from CSV file

# === Exploratory Data Analysis (EDA) ===
print("Dataset Preview:")
print(df.head())  # Preview the first 5 rows of the dataset

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())  # Check if any column has null/missing values

# Clean column names by removing spaces and making them lowercase
df.columns = [col.strip().replace(' ', '_').lower() for col in df.columns]

# === Visualize Data Distribution ===
# Plot distributions of features
df.hist(bins=20, figsize=(10, 8))
plt.suptitle('Feature Distributions')
plt.show()

# Pairplot to visualize relationships between features
sns.pairplot(df, hue='species', palette='viridis')
plt.suptitle('Pairplot of Features', y=1.02)
plt.show()

# Correlation heatmap to check feature correlation
plt.figure(figsize=(8, 6))
sns.heatmap(df.drop('species', axis=1).corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Feature Correlation Heatmap')
plt.show()

# === Encode the target column ===
label_encoder = LabelEncoder()
df['species'] = label_encoder.fit_transform(df['species'])  # Encode the 'species' column

# === Feature Selection ===
# X will contain only the feature columns (input variables), y will be the target ('species')
X = df.drop('species', axis=1)
y = df['species']

# === Train-Test Split ===
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# === Decision Tree Model with Constraints to Reduce Accuracy ===
clf = DecisionTreeClassifier(random_state=42, 
                              max_depth=2,           # Limit the depth of the tree
                              min_samples_split=10,  # Require at least 10 samples for a split
                              min_samples_leaf=5,    # Require at least 5 samples for a leaf node
                              min_impurity_decrease=0.01)  # Increase the impurity decrease threshold

clf.fit(X_train, y_train)  # Train the classifier on the training data

# === Predictions ===
y_pred = clf.predict(X_test)  # Use the trained model to predict on the test data

# === Evaluation ===
# Print the accuracy score
print("\nAccuracy Score:", accuracy_score(y_test, y_pred))

# Print the classification report (precision, recall, f1-score, etc.)
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# === Plot the decision tree ===
plt.figure(figsize=(15, 10))
plot_tree(clf, 
          filled=True, 
          feature_names=X.columns, 
          class_names=label_encoder.inverse_transform(clf.classes_),  # Inverse transform to get class labels as strings
          rounded=True)  # Optional: adds rounded corners to nodes for better visuals
plt.show()